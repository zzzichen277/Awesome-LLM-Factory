

#### prefix LM 和 causal LM 区别是什么?

prefix LM (前缀语言模型)：在输入序列的开头添加一个可学习的任务相关的前缀，然后使用这个前缀和输入序列一起生成输出。这种方法可以引导模型生成适应特定任务的输出。causal LM (因果语言模型)：也称为自回归语言模型，它根据之前生成的 token 预测下一个token。在生成文本时，模型只能根据已经生成的部分生成后续部分，不能访问未来的信息。


#### prefix LM 和 causal LM、encoder-decoder 区别及各自有什么优缺点?

prefix LM：通过在输入序列前添加可学习的任务相关前缀，引导模型生成适应特定任务的输 出。优点是可以减少对预训练模型参数的修改，降低过拟合风险；缺点是可能受到前缀表示长度的限制，无法充分捕捉任务相关的信息。

causal LM：根据之前生成的 token预测下一个 token, 可以生成连贯的文本。优点是可以生成灵 活的文本，适应各种生成任务；缺点是无法访问未来的信息，可能生成不一致或有误的内容。

encoder-decoder：由编码器和解码器组成，编码器将输入序列编码为固定长度的向量，解码器 根据编码器的输出生成输出序列。优点是可以处理输入和输出序列不同长度的任务，如机器翻译；缺点是模型结构较为复杂，训练和推理计算量较大。

#### 大模型的Tokenizer的实现方法及原理?

大模型的Tokenizer通常使用字节对编码 (Byte-Pair Encoding,BPE) 算法。BPE算法通过迭代地将最频繁出现的字节对合并成新的符号，来构建一个词汇表。在训练过程中，模型会学习这些符号的嵌入表示。Tokenizer将输入文本分割成符号序列，然后将其转换为模型可以处理的数字表示。


#### ChatGLM3的词表实现方法?

ChatGLM3 使用了一种改进的词表实现方法。它首先使用字节对编码 (BPE) 算法构建一个基本的词表，然后在训练过程中通过不断更新词表来引入新的词汇。具体来说，ChatGLM3 在训练 过程中会根据输入数据动态地合并出现频率较高的字节对，从而形成新的词汇。这样可以有效地处理大量文本数据，并减少词汇表的规模。同时，ChatGLM3 还使用了一种特殊的词表分割方法，将词表分为多个片段，并在训练过程中逐步更新这些片段，以提高模型的泛化能力和适应性。


#### GPT3、LLAMA、ChatGLM 的 Layer Normalization 的区别是什么?各自的优缺点是什么?

GPT3：采用了Post-Layer Normalization (后标准化)的结构，即先进行自注意力或前馈神经网络的计算，然后进行Layer Normalization。这种结构有助于稳定训练过程，提高模型性能。

LLAMA：采用了Pre-Layer Normalization (前标准化)的结构，即先进行Layer Normalization,然后进行自注意力或前馈神经网络的计算。这种结构有助于提高模型的泛化能力和鲁棒性。

ChatGLM：采用了Post-Layer Normalization的结构，类似于GPT3。这种结构可以提高模型的性能和稳定性。


#### 大模型常用的激活函数有哪些？

ReLU（Rectified Linear Unit）：一种简单的激活函数，可以解决梯度消失问题，加快训练速度。
GeLU（Gaussian Error Linear Unit）：一种改进的ReLU函数，可以提供更好的性能和泛化能力。
Swish：一种自门控激活函数，可以提供非线性变换，并具有平滑和非单调的特性。


#### 多查询注意力与群查询注意力是否了解?区别是什么?

Multi-query Attention 和 Grouped-query Attention 是两种不同的注意力机制变种，用于改进和扩展传统的自注意力机制。

Multi-query Attention：在Multi-query Attention中，每个查询可以与多个键值对进行交互，从而 捕捉更多的上下文信息。这种机制可以提高模型的表达能力和性能，特别是在处理长序列或复杂关系时。

Grouped-query Attention：在Grouped-query Attention中，查询被分成多个组，每个组内的查询与对应的键值对进行交互。这种机制可以减少计算复杂度，提高效率，同时仍然保持较好的性能。



#### 下面是面试过程中记录的一些关于大模型、强化学习的问题，仅供参考。

PPO算法中使用GAE的好处以及参数γ和λ的作用是什么？
PPO算法和DQN算法的区别是什么？
有哪些PPO算法的调参经验？
在线强化学习和离线强化学习在技术和应用场景上有什么区别？
强化学习和大模型之间的关联是什么？
如何评估大模型中数据集的质量？
目前国内一般选择基于哪些基座模型继续训练？
国内做大模型的主要工作是哪几个部分？
除了数据之外，还有哪些方向的工作可以进一步优化大模型的效果？
大语言模型是怎么输出的，观察过输出的概率值吗？
关于微调的方法有哪些？
如果让你训练一个模型，基座，数据，finetune的方法怎么选？
怎么解决大语言模型的幻觉问题，RLHF可以吗？
是否看好国内做基座模型工作的前景，为什么？
为什么模型越大，貌似更多地具备AGI的能力？这背后的逻辑是什么？
介绍下对transformer的了解，网络结构相比于lstm有什么不同？
transformer里用到的正则化方法有哪些？
chatgpt训练过程中，奖励模型有更新吗？
chatgpt强化学习训练阶段还有什么改进的空间和思路吗？
直接用训练reward model的数据精调模型，而不用强化学习，是否可行？为什么？
了解bert和gpt网络结构的细节及其差异吗？
假如reward model不太准，怎么办？
有做过大模型训练的实践吗，有哪些收获或者感悟？
坦白讲，这些问题对于有过大模型项目经验的人来说应该问题不大，尤其是有NLP背景的。但是如果目前工作内容和大模型无关，想要比较好的回答出上述问题还是有难度的。要准备好八股文，要对前沿的方向有了解，而且最好自己跑过大模型的训练，包括微调和对齐。



#### 一些比较高频的东西（针对基座算法/框架岗位为主，大体按重要性排序）：

1.多头注意力，频率太高了。coding轮，概念轮都考。复习的点包括：时间/空间复杂度，优化（kv-cache，MQA，GQA），手写多头代码。各种Norm，这个频率也不低，不过比较标准的内容，没有啥特意要说的，有的考手写，有的考概念和理解（为什么管用）。

2.框架相关内容，各种并行方式，优缺点。DeepSpeed，Megatron可以看看源代码，Flash-Attention等内容。这个点也经常考代码题。

3.BERT，GPT等比较主流大模型，一些细节，比如位置编码，训练loss，激活，架构些许不同这种。自回归重点。

4.大模型训练，这个可能主要是工作经验相关，经常问比如训练loss炸掉了，如何解决，一些技巧之类的。面试时有些面试官会问一些很细节的东西，感觉是在确认确实上手跑过基座训练不是吹水。

5.数据预处理，BPE，tokenization，mask相关概念和对模型/训练影响，数据配比（有paper）。

6.evaluation，如何评估大模型，安全性，有效性，公开数据，个别考过手写eval框架（多选，生成）。

7.根据投的岗位，多模态和RLHF内容可以适当看看。这俩感觉paper挺重要的，也大多研究岗位。楼主也少面了一些自动驾驶，RL啥的，不过结果不咋地。

https://zhuanlan.zhihu.com/p/657826357


#### LoRA 的思路是什么？

LoRA的实现思想很简单，就是冻结一个预训练模型的矩阵参数，并选择用A和B矩阵来替代，在下游任务时只更新A和B。

<br>1. 在原模型旁边增加一个旁路，通过低秩分解（先降维再升维）来模拟参数的更新量；
<br>2. 训练时，原模型固定，只训练降维矩阵A和升维矩阵B；
<br>3. 推理时，可将BA加到原参数上，不引入额外的推理延迟；
<br>4. 初始化，A采用高斯分布初始化，B初始化为全0，保证训练开始时旁路为0矩阵；
<br>5. 可插拔式的切换任务，当前任务W0+B1A1，将lora部分减掉，换成B2A2，即可实现任务切换；

参考：【LLM模型微调】LLMs-PEFT[微调]-LoRA总结笔记v5.0-https://mp.weixin.qq.com/s/ltwEH5kLOjJ_OGys57vH9g

#### QLoRA 的思路是什么？
QLORA训练过程跟LORA基本上是一致的。区别在于QLORA模型是按照NF4保存的，训练时需要把参数反量化到bf16后进行训练。QLORA 有低精度存储数据类型（NF4）+计算数据类型（BFloat16）。

QLoRA使用以下技术—NF4量化 +双量化[实现高效4bit微调] +分页优化器[防止内存不足]。

具体说明如下：
<br>1.4bit NormalFloat（NF4）：4-bit NormlFLoat量化是对Quantile Quantization(分位量化)进行了改进，并结合Block-wise Quantization，降低计算复杂度和误差。即提出基于分块(Block-wise)的分位数量化(Quantile Quantization)的量化策略；
<br>2.双重量化（Double Quantization）：是针对量化常数的二次量化。由于BnB的量化是块量化（block-wise），因此块级别的常数存储也会占用GPU memory。对第一次量化后的那些常量再进行一次量化，减少存储空间。
<br>3.分页优化器（Paged Optimizers）：为防止梯度检查点所引起的内存波动，导致的内存不足错误。使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动的页面切换，以实现无错误的 GPU 处理。使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。

参考：【LLM模型微调】LLMs-PEFT[微调]-QLoRA总结笔记v6.0-https://mp.weixin.qq.com/s/_6axtPpfsqECVb9dgbcz0g



#### LoRA是否适用于领域适应？

答：LoRA通常用于引导LLM遵循指令，而不是从预训练数据集中吸收知识。在内存有限时，可以用LoRA对特定领域数据集进行进一步的预训练。

#### LoRA 高效微调如何避免过拟合？

过拟合还是比较容易出现的。减小r或增加数据集大小可以帮助减少过拟合，还可以尝试增加优化器的权重衰减率或LoRA层的dropout值。

#### LoRA微调方法为啥能加速训练？

<br>1）只更新了部分参数：比如LoRA原论文就选择只更新Self Attention的参数，实际使用时我们还可以选择只更新部分层的参数；
<br>2）减少了通信时间：由于更新的参数量变少了，所以（尤其是多卡训练时）要传输的数据量也变少了，从而减少了传输时间；
<br>3）采用了各种低精度加速技术，如FP16、FP8或者INT8量化等。LoRA的优点是它的低秩分解很直观，在不少场景下跟全量微调的效果一致，以及在预测阶段不增加推理成本。


#### 是否可以逐层调整LoRA的最优rank？

理论上，可以为不同层选择不同的LoRA rank，类似于为不同层设定不同学习率，但由于增加了调优复杂性，实际中很少执行。

#### LoRA 微调参数量怎么确定？

LoRA 模型中可训练参数的结果数量取决于低秩更新矩阵的大小，其主要由秩 r 和原始权重矩阵的形状确定。实际使用过程中，通过选择不同的 lora_target 决定训练的参数量。 
以 LLama 为例： --lora_target q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

#### 如何选择最佳的LoRA的参数r？

rank的取值比较常见的是8，理论上说rank在4-8之间效果最好，再高并没有效果提升。不过论文的实验是面向下游单一监督任务的，因此在指令微调上根据指令分布的广度，rank选择还是需要在8以上的取值进行测试。选择LoRA的参数r是个需要实验的超参数，太大可能导致过拟合，太小可能不足以处理数据集中的任务多样性。

#### alpha参数 如何选取？

alpha用于缩放学习到的权重。现有文献，包括原始LoRA论文，通常建议将Alpha固定在16，而不是将其作为可调超参数进行调整。本质和learning rate相同，好的经验法则是默认让alpha=2*rank，只调整lr，这样可以简化超参。

#### LoRA 高效微调如何避免过拟合？

过拟合还是比较容易出现的。减小r或增加数据集大小可以帮助减少过拟合，还可以尝试增加优化器的权重衰减率或LoRA层的dropout值。

#### LoRA权重是否可以合并？ 

可以将多套LoRA权重合并。训练中保持LoRA权重独立，并在前向传播时添加，训练后可以合并权重以简化操作。训练好的低秩矩阵（B*A）+原模型权重合并（相加），计算出新的权重。

#### 如何在已有LoRA模型上继续训练？

理解此问题的情形是：已有的lora模型只训练了一部分数据，要训练另一部分数据的话，是在这个lora上继续训练呢，还是跟base 模型合并后再套一层lora，或者从头开始训练一个lora？ 

把之前的LoRA跟base model 合并后，继续训练就可以，为了保留之前的知识和能力，训练新的LoRA时，加入一些之前的训练数据是需要的。每次都要重头训练的话成本比较高。

#### 是否需要在所有层启用LoRA？

目前的论文研究仅限于在Attention的query和key权重矩阵启用LoRA以及在所有层启用。未来的实验可以探索其他层组合的影响。如果你在使用 LoRA，确保它应用于所有层，而不仅仅是 Key 和 Value 矩阵，以最大化模型性能。

#### 哪些因素会影响内存使用？
答：内存使用受到模型大小、批量大小、LoRA参数数量以及数据集特性的影响。例如，使用较短的训练序列可以节省内存。

#### 其他优化器如何？

答：除了Adam和AdamW，其他优化器如Sophia也值得研究，它使用梯度曲率而非方差进行归一化，可能提高训练效率和模型性能。
#### ChatGLM-6B LoRA后的权重多大？

rank=8 target_module=query，key，value条件下，大约15M。

#### LoRA与完全微调或RLHF比较如何？

答：虽未进行RLHF实验，但全微调需要更多资源，且可能因过拟合或非理想超参数而性能不佳。


#### LoRA这种微调方法和全参数比起来有什么劣势吗？

LoRA中参与训练的参数量较少，解空间较小，效果相比全量微调有一定的差距。

![image](https://github.com/user-attachments/assets/4a061a70-fcbd-4914-94a9-fb68ffe827ba)

#### LORA应该作用于Transformer的哪个参数矩阵？
![image](https://github.com/user-attachments/assets/355f3365-8964-4ccb-979d-41ddce1bf8f4)

1）将所有微调参数都放到attention的某一个参数矩阵的效果并不好，将可微调参数平均分配到 Wq 和 Wk 的效果最好；

2）即使是秩仅取4也能在 ∆W 中获得足够的信息。在实际操作中，应当将可微调参数分配到多种类型权重矩阵中，而不应该用更大的秩单独微调某种类型的权重矩阵。

#### Lora的矩阵怎么初始化？
<br>如果A用随机高斯分布初始化，B用0矩阵初始化，那么可以保证训练的开始右旁路矩阵∆W依然是0矩阵，这样模块的输出就基本上来自于左路W，也就是大模型原有参数的计算结果，这使得模型优化的初始点和原始的大模型保持一致。
<br>如果将A和B都用0矩阵初始化，那么在训练初期，模型将无法学习到有效的特征表示，很容易导致梯度消失，导致微调效果不佳。
<br>如果A用0矩阵初始化，B用随机高斯分布初始化，那么在训练初期，模型可能会受到B矩阵的干扰，导致训练过程不稳定，甚至可能损害模型的性能。
<br>如果B，A全部随机高斯分布初始化，那么在网络训练刚开始就会有概率为得到一个过大的偏移值ΔW 从而引入太多噪声，导致难以收敛。
<br>总之，LoRA中降维矩阵A和升维矩阵B的初始化方式是为了保持模型的表达能力，同时减少对原始模型参数的影响，从而提高微调的效率和稳定性。在实际应用中，可以根据具体任务和模型需求，对初始化方式进行适当调整。



