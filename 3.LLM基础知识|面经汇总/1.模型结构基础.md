

#### prefix LM 和 causal LM 区别是什么?

prefix LM (前缀语言模型)：在输入序列的开头添加一个可学习的任务相关的前缀，然后使用这个前缀和输入序列一起生成输出。这种方法可以引导模型生成适应特定任务的输出。causal LM (因果语言模型)：也称为自回归语言模型，它根据之前生成的 token 预测下一个token。在生成文本时，模型只能根据已经生成的部分生成后续部分，不能访问未来的信息。


#### prefix LM 和 causal LM、encoder-decoder 区别及各自有什么优缺点?

prefix LM：通过在输入序列前添加可学习的任务相关前缀，引导模型生成适应特定任务的输 出。优点是可以减少对预训练模型参数的修改，降低过拟合风险；缺点是可能受到前缀表示长度的限制，无法充分捕捉任务相关的信息。

causal LM：根据之前生成的 token预测下一个 token, 可以生成连贯的文本。优点是可以生成灵 活的文本，适应各种生成任务；缺点是无法访问未来的信息，可能生成不一致或有误的内容。

encoder-decoder：由编码器和解码器组成，编码器将输入序列编码为固定长度的向量，解码器 根据编码器的输出生成输出序列。优点是可以处理输入和输出序列不同长度的任务，如机器翻译；缺点是模型结构较为复杂，训练和推理计算量较大。

#### 大模型的Tokenizer的实现方法及原理?

大模型的Tokenizer通常使用字节对编码 (Byte-Pair Encoding,BPE) 算法。BPE算法通过迭代地将最频繁出现的字节对合并成新的符号，来构建一个词汇表。在训练过程中，模型会学习这些符号的嵌入表示。Tokenizer将输入文本分割成符号序列，然后将其转换为模型可以处理的数字表示。


#### ChatGLM3的词表实现方法?

ChatGLM3 使用了一种改进的词表实现方法。它首先使用字节对编码 (BPE) 算法构建一个基本的词表，然后在训练过程中通过不断更新词表来引入新的词汇。具体来说，ChatGLM3 在训练 过程中会根据输入数据动态地合并出现频率较高的字节对，从而形成新的词汇。这样可以有效地处理大量文本数据，并减少词汇表的规模。同时，ChatGLM3 还使用了一种特殊的词表分割方法，将词表分为多个片段，并在训练过程中逐步更新这些片段，以提高模型的泛化能力和适应性。


#### GPT3、LLAMA、ChatGLM 的 Layer Normalization 的区别是什么?各自的优缺点是什么?

GPT3：采用了Post-Layer Normalization (后标准化)的结构，即先进行自注意力或前馈神经网络的计算，然后进行Layer Normalization。这种结构有助于稳定训练过程，提高模型性能。

LLAMA：采用了Pre-Layer Normalization (前标准化)的结构，即先进行Layer Normalization,然后进行自注意力或前馈神经网络的计算。这种结构有助于提高模型的泛化能力和鲁棒性。

ChatGLM：采用了Post-Layer Normalization的结构，类似于GPT3。这种结构可以提高模型的性能和稳定性。


#### 大模型常用的激活函数有哪些？

ReLU（Rectified Linear Unit）：一种简单的激活函数，可以解决梯度消失问题，加快训练速度。
GeLU（Gaussian Error Linear Unit）：一种改进的ReLU函数，可以提供更好的性能和泛化能力。
Swish：一种自门控激活函数，可以提供非线性变换，并具有平滑和非单调的特性。


#### 多查询注意力与群查询注意力是否了解?区别是什么?

Multi-query Attention 和 Grouped-query Attention 是两种不同的注意力机制变种，用于改进和扩展传统的自注意力机制。

Multi-query Attention：在Multi-query Attention中，每个查询可以与多个键值对进行交互，从而 捕捉更多的上下文信息。这种机制可以提高模型的表达能力和性能，特别是在处理长序列或复杂关系时。

Grouped-query Attention：在Grouped-query Attention中，查询被分成多个组，每个组内的查询与对应的键值对进行交互。这种机制可以减少计算复杂度，提高效率，同时仍然保持较好的性能。



#### 下面是面试过程中记录的一些关于大模型、强化学习的问题，仅供参考。

PPO算法中使用GAE的好处以及参数γ和λ的作用是什么？
PPO算法和DQN算法的区别是什么？
有哪些PPO算法的调参经验？
在线强化学习和离线强化学习在技术和应用场景上有什么区别？
强化学习和大模型之间的关联是什么？
如何评估大模型中数据集的质量？
目前国内一般选择基于哪些基座模型继续训练？
国内做大模型的主要工作是哪几个部分？
除了数据之外，还有哪些方向的工作可以进一步优化大模型的效果？
大语言模型是怎么输出的，观察过输出的概率值吗？
关于微调的方法有哪些？
如果让你训练一个模型，基座，数据，finetune的方法怎么选？
怎么解决大语言模型的幻觉问题，RLHF可以吗？
是否看好国内做基座模型工作的前景，为什么？
为什么模型越大，貌似更多地具备AGI的能力？这背后的逻辑是什么？
介绍下对transformer的了解，网络结构相比于lstm有什么不同？
transformer里用到的正则化方法有哪些？
chatgpt训练过程中，奖励模型有更新吗？
chatgpt强化学习训练阶段还有什么改进的空间和思路吗？
直接用训练reward model的数据精调模型，而不用强化学习，是否可行？为什么？
了解bert和gpt网络结构的细节及其差异吗？
假如reward model不太准，怎么办？
有做过大模型训练的实践吗，有哪些收获或者感悟？
坦白讲，这些问题对于有过大模型项目经验的人来说应该问题不大，尤其是有NLP背景的。但是如果目前工作内容和大模型无关，想要比较好的回答出上述问题还是有难度的。要准备好八股文，要对前沿的方向有了解，而且最好自己跑过大模型的训练，包括微调和对齐。



#### 一些比较高频的东西（针对基座算法/框架岗位为主，大体按重要性排序）：

1.多头注意力，频率太高了。coding轮，概念轮都考。复习的点包括：时间/空间复杂度，优化（kv-cache，MQA，GQA），手写多头代码。各种Norm，这个频率也不低，不过比较标准的内容，没有啥特意要说的，有的考手写，有的考概念和理解（为什么管用）。

2.框架相关内容，各种并行方式，优缺点。DeepSpeed，Megatron可以看看源代码，Flash-Attention等内容。这个点也经常考代码题。

3.BERT，GPT等比较主流大模型，一些细节，比如位置编码，训练loss，激活，架构些许不同这种。自回归重点。

4.大模型训练，这个可能主要是工作经验相关，经常问比如训练loss炸掉了，如何解决，一些技巧之类的。面试时有些面试官会问一些很细节的东西，感觉是在确认确实上手跑过基座训练不是吹水。

5.数据预处理，BPE，tokenization，mask相关概念和对模型/训练影响，数据配比（有paper）。

6.evaluation，如何评估大模型，安全性，有效性，公开数据，个别考过手写eval框架（多选，生成）。

7.根据投的岗位，多模态和RLHF内容可以适当看看。这俩感觉paper挺重要的，也大多研究岗位。楼主也少面了一些自动驾驶，RL啥的，不过结果不咋地。

https://zhuanlan.zhihu.com/p/657826357
