### 1、在Transformer模型中，为什么scaled dot-product attention在计算QK内积之后要除以根号d？

简单来说，就是需要压缩softmax输入值，以免输入值过大，进入了softmax的饱和区，导致梯度值太小而难以训练。如果不对attention值进行scaling，也可以通过在参数初始化时将方差除以根号d ，同样可以起到预防softmax饱和的效果。

### 2、Transformer自注意力计算中，为什么Q和K要使用不同的权重矩阵进行线性变换投影，为什么不使用同一个变换矩阵，或者不进行变换？

1、如果Q和K一样，则矩阵乘积的结果是一个对称矩阵，这样减弱了模型的表达能力。2、如果Q和K一样，乘积结果的对称矩阵中，对角线的值会比较大，导致每个位置过分关注自己。

3、使用不同的投影矩阵，参数增多，可以增强模型表达能力。

### 3、Transformer模型中，注意力计算后面使用了两个FFN层，为什么第一个FFN层先把维提升，第二个FFN层再把维度降回原大小？

1、提升维度：类似SVM kernel，通过提升维度可以识别一些在低维无法识别的特征。2、提升维度：更大的可训练参数，提升模型的容量。
3、降回原维度：方便多层注意力层和残差模块进行拼接，而无需进行额外的处理。

### 4、MQA(Multi-Query Attention)和GQA(Grouped-Query Attention)相比MHA(Multi-Head Attention)，计算量变化如何，主要带来了什么优化？

1、MQA和GQA虽然可训练参数量比MHA少，但是计算量和MHA相比变化不大，主要在生成KV时有少量降低。2、Decoder-only的大模型由于causal attention的存在，使用了KV缓存加速推理。MQA和GQA能减少KV头的数量，节省了缓存，使得在输入长度较长时也能把KV放进缓存。

### 5、为什么现在主流的LLM模型基本都是Decoder-only的结构？单向注意力模型为什么效果比双向注意力效果好？

1、双向Attention在多层模型训练中容易退化成低秩矩阵，限制了模型容量；而Decoder-only模型使用了下三角注意力矩阵，使得训练过程中矩阵是满秩，建模能力更强。

2、单向注意力模型相比双向注意力模型在训练的时候难度更大，能迫使模型学到更多信息。3、Causal Attention天然具有位置编码的功能，而双向Attention即使交换两个token的位置也基本不影响表示，对语序区分能力较弱。

4、工程上，单向模型支持KV Cache等，对于对话场景效率友好。5、轨迹依赖，基模型训练成本高，业界倾向于沿着已经成功的模型继续开发。


https://mp.weixin.qq.com/s/DXOKLkXdTFfANpvV4eiQNA


### 1、在Bert中，词向量token embedding和(绝对)位置编码position encoding为什么可以直接相加？

1、两个向量相加，理论上其效果等价于维度拼接concat+线性变换，而相加的操作在实现上更为方便。
2、高维空间中(如768维)，两个随机向量近似为正交关系。模型在高维度有能力区分出所有组合的情况。假设共有2万个词向量，500个位置，则模型需要在768维空间区分1000万个点，即使768维每个维度只能取1和-1也具备足够的区分能力。
3、词向量和位置编码可以认为都是一个one-hot向量经过一层线性变换层得到的。两个向量相加等价于把它们的one-hot编码拼接后进行线性变换。
4、没有使用相乘则是出于工程考虑。相加相比相乘结果更为稳定，方便训练。


### 2、LoRA和全参数训练在计算量和显存上相比如何？为什么LoRA能提升大模型训练效率？

1、计算量上：LoRA训练时，在主干模型的（部分）全连接层增加了LoRA旁路，前向和后向的计算量都在主干模型的基础上，增加了旁路部分的计算，因此相比全参数训练，略有增加。
2、显存上：训练时，显存主要有①模型参数②梯度③中间激活值④优化器参数四个部分。模型参数/梯度/激活值相比全参数训练也略微增加；而优化器则不需要再存储原模型参数的部分，只需要存储LoRA旁路部分，这部分节省较多显存。
3、使用LoRA能提升训练效率主要是因为（1）优化器部分的显存需要减少了，可以增大batch（2）优化器参数减少了，分布式训练中多卡之间的通信量减少了（3）（optional）主干模型由于不用更新，可以进一步量化到int8/int4等。

### 3、为什么模型需要normalization（batchnorm/layernorm等）？
1、输入数据包含多个特征，特征之间有不同的量纲和范围（如身高180和年龄18岁），通过normalization进行归一化再经过模型进行线性/非线性组合，能够防止部分特征占据主导，部分特征被忽略。
2、batchnorm论文认为：模型一般有多层，前一层的输出是后一层的输入，而训练中前一层的参数更新会导致后一层的输入数据分布变化导致ICS（internal covariate shift），这样后面的层就不得不频繁剧烈更新适应分布变化，导致分布偏移进入激活函数饱和区而出现梯度消失，另外分布变化也是对i.i.d.条件的破坏。使用normalization可以保持分布的稳定，减小方差，使模型训练可以正常进行。
3.《How Does Batch Normalization Help Optimization?》设计了实验测量使用batchnorm前后的ICS，发现batchnorm实际上并没有缓解ICS，甚至有所增加。而batchnorm能优化模型训练的原因更多是使得损失函数平面更加光滑，而便于梯度下降收敛。

### 4、Transformer中pre-norm和post-norm各有什么优缺点?

1.原始的Transformer用的是post-norm，它在残差之后进行归一化（add & norm），对参数正则化的效果更强，模型更为鲁棒；post-norm对每个通路都进行了归一化，使得梯度在回传的时候容易出现消失。
2.Pre-norm相对于post-norm，残差部分存在不经过归一化的通路，因此能够缓解梯度消失，能够训练更深的网络。但是模型的等效“深度”受到影响，L+1层网络近似于一个L层的宽网络。
3.也就是说，在层数较少，post-norm和pre-norm都能正常收敛的情况下，post-norm的效果更好一些；但是pre-norm更适合用于训练更深的网络。

### 5、对于使用Multi-Head Attention的模型，假设hidden size=D，注意力头数量为h，每个头维度为d（假设有D=d×h），输入上下文长度为s，batch size=1，计算self-attention模块各个部分的计算量（Float Operations）。
1.QKV线性变换：6 × s × D^2（矩阵乘法，每个位置有加法和乘法两个运算，因此每个位置需要2D次计算）
2.QK内积：h × 2 × d × s^2（h组矩阵分别计算）
3.scaling：h × s^24.softmax：h × 3 × s^2（softmax是按列进行的，每列要计算s个exp，s个exp结果的求和，以及s次除法）5.reduction（权重矩阵乘以V）：h × 2 × d × s^2

https://mp.weixin.qq.com/s/J4sDsCS4izbHd0lKXPOz9A




### 1.旋转位置编码RoPE有什么优缺点？
优点：RoPE以绝对位置编码的方式实现了相对位置编码，使得能够在不破坏注意力形式的情况下，以“加性编码”的方式让模型学习相对位置。
①相比其他相对位置编码来说，实现简单，计算量少。②可以应用于线性注意力。
③RoPE具有远程衰减的特性，使得每个位置天然能够更关注到附近的信息。缺点：RoPE相比训练式的绝对位置编码具有一定的外推能力，如可以在2k数据长度训练的模型进行略长于2k的推理。但是相比于Alibi等位置编码，其直接外推能力并不算特别好，需要通过线性插值、NTK插值、YaRN等方式来优化外推能力。


### 2.batchnorm中的momentum怎么影响训练效果?

batchnorm在训练时计算每个batch内的均值和方差用于normalization，同时统计一个全局均值和方差用于推理。

全局均值和方差计算公式为：moving_mean = momentum × moving_mean + (1.0 − momentum) × meanmoving_var = momentum × moving_var + (1.0 − momentum) × var小的momentum值对应快的更新速度，能够更快地向真实分布靠近，但是同时也会导致更大的波动；

如果更新过慢，则可能导致训练结束时还没有统计到真实的分布，是欠拟合的状态。如果batch size比较小，每个mini batch和全局差异较大，就不应该用太大的momentum。

### 3.多头注意力相比单头有什么好处？

多头注意力使用多个维度较低的子空间分别进行学习。一般来说，相比单头的情况，多个头能够分别关注到不同的特征，增强了表达能力。多个头中，会有部分头能够学习到更高级的特征，并减少注意力权重对角线值过大的情况。
比如部分头关注语法信息，部分头关注知识内容，部分头关注近距离文本，部分头关注远距离文本，这样减少信息缺失，提升模型容量。另外虽然多头注意力的整体计算量比单头要大一点，但是并行度也高一些。

### 4.kv cache为什么能加速推理？

对于GPT类模型，使用的是单向注意力，每个位置只能看到自己和前面的内容。在进行自回归解码的时候，新生成的token会加入序列，一起作为下一次解码的输入。
由于单向注意力的存在，新加入的token并不会影响前面序列的计算，因此可以把已经计算过的每层的kv值保存起来，这样就节省了和本次生成无关的计算量。
通过把kv值存储在速度远快于显存的L2缓存中，可以大大减少kv值的保存和读取，这样就极大加快了模型推理的速度。

### 5.ReLU有什么优缺点？

优点：（1）计算快，前向只需要进行max(0, x)计算，后向则是直接透传；（2）有激活值的时候，梯度恒定为1，不会爆炸/消失；缺点：（1）均值不为0，分布产生偏移（2）输入值小于0时，梯度再也无法回传过来，导致神经元坏死。

https://mp.weixin.qq.com/s/qvNz5-FJP05d3HMdyEJeww

### 1.为什么Transformer用layernorm而不是batchnorm

首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。
直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。
《PowerNorm: Rethinking Batch Normalization in Transformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。

### 2.transformer中，encdoer和decoder是怎么进行交互的？

decoder部分的输入，在每层中，先进行一次self-attention；之后用encoder的输出作为attention计算中的K、V，decoder的输入作为Q，进行cross-attention。

### 3.PyTorch中，Tensor的view()和reshape()两个方法有什么区别？

1.功能上：view()与reshape()方法都可以用来改变tensor的形状，但是使用条件不同，view()能做的是reshape的子集。
2.view()方法需要tensor满足连续性，操作后返回一个引用，返回值是视图，没有改变原储存空间的值，多个视图共享同一个物理储存空间的内容。
3.reshape()方法不需要tensor一定满足连续性。如果tensor不满足连续性的要求，则会使用新的储存空间并返回。如果满足连续性需求，则功能和view()一致。
4.连续性：比如一个二维张量，如果按行优先展开成一维的结果，和物理储存顺序是一致的，就是连续的。
可以用is_contiguous()来判断一个张量是否连续，如果不连续，可以用contiguous()得到一份新空间中的连续副本。

### 4.RLHF中，PPO需要哪几个模型，分别是什么作用？

一般来说，PPO需要使用4个模型。1.Actor模型：由SFT初始化，就是进行强化学习的主模型，是我们想要最终获得的模型；它不断产生action并被Critic模型所评价，计算loss进行训练。
2.Reference模型：一般也是从SFT模型初始化，RLHF中Reference模型并不更新参数，只是作为Actor模型的参考使用；通过约束Actor模型和Reference模型的KL penalty等，可以防止Actor模型被训得跑得太偏。
3.Reward模型：提前训练好的，对SFT模型进行打分的模型，RLHF中参数是冻结的。
4.Critic模型：一般由Reward模型进行初始化，参数可训练，用于预测Actor模型生成的token的收益。

### 5.GPT类模型训练过程中，消耗显存的主要有哪些部分？分别是多少？哪部分占用最多？
假设模型有L层，词表大小为V，hidden size为H，batch size为B，训练窗口长度为S，使用Adam优化器混合精度训练（需要存一阶和二阶动量），注意力头数为N。

训练过程中，显存消耗主要有模型参数、梯度、optimizer状态值和中间激活值。

1.模型参数Φ：词表部分VH，每层参数12H^2+13H，总共有Φ=VH+L(12H^2+13H)，如果是半精度就是2Φ
2.梯度：每个参数对应有一个梯度，总量为Φ，如果是半精度就是2Φ
3.optimizer状态值：每个参数有一个对应梯度，每个参数又对应优化器一个一阶动量和二阶动量。在混合精度训练中，使用半精度进行前向计算和梯度计算，同时优化器备份一份单精度的优化器状态、梯度和参数用于更新参数，因此共有(Φ+Φ)*2+(Φ+Φ+2Φ)*4=20Φ，除去参数和梯度，优化器占部分16Φ
4.激活值：保存激活值是为了计算梯度，因此每个矩阵相乘、softmax、dropout都需要保存输入值的中间的激活值。总共是34BSH+5BNS^2，如果都是半精度，就乘以2。模型参数、梯度和优化器状态和输入长度无关，是固定值，而激活值随着长度增加，以平方速度增长。
以GPT3（175B）为例，H=12288，L=96，N=96。模型参数量显存越为350G。以B=1计算，如果S=1024，激活值约为90G；如果S=8192，激活值约为3420G。

https://mp.weixin.qq.com/s/IJL5XmwuIaCiuoEhuLaPMw


### 1.使用半精度训练时，bf16和fp16格式有什么异同？

二者都是占用16bit空间。fp16由1个符号位、5个指数位和10个尾数位组成。fp16在表达小数时具有较高的精度，但表示的最大范围相对bf16比较小。

相比bf16，在表达较大的数时更容易出现上溢的情况。bf16由1个符号位、8个指数位和7个尾数位组成。
相比于fp16，bf16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如fp16。
模型训练时使用bf16和fp16都可以降低内存使用和传输量，提高训练效率。

### 2.支持模型长上下文的方案「NTK-aware interpolation」的思路是什么？

1.在NTK插值之前，线性插值通过在原模型训练的两个位置编码中间，插入新的位置编码，使得同样的取值范围可以容纳更多位置。
2.而NTK插值则是一种非线性插值的方法。它通过仅改变RoPE的base，使得位置编码中不同频率的信号有不同的表现，具体来说就是“高频外推，低频内插”。高频信号使用外推，防止分辨率太低，而低频信号沿用插值的方式，实现方便。

### 3.LLM长度外推方案NTK-by-parts的思路是什么？

NTK-by-parts的方法在NTK插值的基础上又多想了一层。它认为无论是线性插值还是NTK-aware插值，都认为RoPE的所有分量都对网络有同样的重要性。
而NTK-by-parts的思路认为，应该区别对待不同分量，他们对网络的影响有所不同。对于波长远小于上下文长度的分量（如波长<=1/32上下文），就不插值只外推；
而对于波长大于等于上下文长度的分量，就只外推不插值；对于介于两者之间的分量，就使用外推和插值的加权和。使用一个斜坡函数来定义NTK-by-parts的分段插值方法，如下所示

### 4.LLM长度外推方案YaRN是怎做的？

PI/NTK/NTK-by-parts主要的做法都是使用插值，而随着插值进行，token之间的距离变得更近（因为现在每一个位置旋转角度变小了），平均最小距离在减小，这样注意力softmax的分布会变得更尖，也就是都集中在某个区间。
换句话说，就是RoPE原本远距离衰减的特性变弱了，衰减得更不明显，就会导致模型更平均地关注到更多的token，这样就削弱了注意力机制，导致输出质量下降。
可以通过在softmax之前，将中间注意力矩阵乘以温度 t>1来缓解这个问题。由于RoPE被编码为一个旋转矩阵，就可以简单地给旋转矩阵乘以一个系数根号t来实现，这样可以不必修改注意力的代码。
YaRN结合NTK-by-parts和这个温度系数，对attention score进行调整。

### 5.对于使用Group-Query Attention的模型，假设hidden size=D，Q的注意力头数量为h，每个头维度为d（假设有D=d×h），kv组数为n，输入上下文长度为s，batch size=b，模型层数为L，计算推理时kv cache所需的空间。

kv cache缓存的是经过投影变换之后的K和V矩阵。对于GQA，每层有n组K和V，每组的特征维度和Q的每个头的特征维度相同，为D/h。
则每层每组K和V数据量为sD/h，整个模型共有2LnsD/h个数据，因此整个batch需要缓存2bLnsD/h个数据。如果使用的是半精度浮点数，每个浮点需要两个字节，因此共需要4bLnsD/h字节的空间。



### 1.Xavier初始化思路是什么，是怎么做的？

2010年的《Understanding the difficulty of training deep feedforward neural networks》提出Xavier初始化，目的是为了保持模型中每一层的输出方差大致相等，这样做的原因主要有：
（1）避免梯度消失或梯度爆炸：在深度神经网络中，如果层与层之间的输出方差相差很大，就会有很大的值出现，那么在反向传播过程中，梯度可能会变得非常小（梯度消失）或者非常大（梯度爆炸）导致梯度更新缓慢，或者训练过程不稳定。
（2）保持信号的传播：通过保持每一层的输出方差大致相等，可以确保网络中的信号在前向传播和反向传播时不会因为方差的变化而减弱或增强，从而有助于网络更好地学习和传递信息。
（3）提高训练的稳定性和效率：当每一层的输出方差保持一致时，训练过程会更加稳定，因为权重更新的magnitude更加可控。这样可以提高训练的效率。
（4）避免过拟合或欠拟合：适当的方差控制有助于网络在训练过程中保持适当的泛化能力。过大的方差可能导致过拟合，而过小的方差可能导致欠拟合。
Xavier初始化方法通过考虑前一层的节点数（fan-in）和后一层的节点数（fan-out）来设置初始权重的分布范围。有三种方案：（1）只考虑输入，设置参数的方差为1/fan-in（2）只考虑输出，设置参数的方差为1/fan-out
（3）同时考虑输入和输出，设置参数的方差为2/（fan-in + fan-out）。实际使用中1/fan-in效果就较好。


### 2.RLHF中，Reward模型和Critic模型的作用分别是什么？

Critic模型主要负责评估当前策略下的行为，并预测未来的回报。在PPO中，Critic模型的输出用于计算优势函数（advantage function），这是一个衡量实际回报与预期回报之间差异的指标。
优势函数是策略梯度方法中的关键组成部分，它帮助actor模型了解哪些行为比预期要好，哪些行为比预期要差。因此，Critic模型的评分直接影响actor模型的优化过程，指导其调整行为以提高整体性能。
Reward模型是基于人类反馈训练的，用于评估和打分模型生成的文本或行为的质量。在RLHF中，Reward模型的评分通常被用作奖励信号，直接反馈给Actor模型。
这些奖励信号反映了人类对生成内容的偏好和评价标准，Actor模型会根据这些信号调整其行为，以生成更符合人类期望的输出。RLHF中，Actor模型的优化目标是最大化期望回报。
这个期望回报可以由Critic模型的预测和Reward模型的评分共同决定。
具体来说，Actor模型的优化目标（objective）通常包括（1）策略损失：这部分损失来自于策略模型尝试最大化奖励信号（由Reward模型提供）和/或优势函数（由Critic模型提供）。
（2）价值损失：在Actor-Critic架构中，Actor模型也负责优化Critic模型，那么价值损失会尝试最小化Critic模型预测的价值与实际回报之间的差异。
（3）其他正则化项：可能还包括一些正则化项。因此，Critic模型的评分用于计算优势函数，而Reward模型的评分直接作为奖励信号。
这两个模型共同作用于Actor模型，帮助其学习如何生成更符合人类偏好和期望的行为。

### 3.Kaiming初始化是怎么做的？

Kaiming初始化在Xavier初始化的基础上做了调整。Xavier初始化的推导是基于线性函数的，但实际上今天所有模型都会大量使用非线性函数，比如ReLU，这样就导致了Xavier初始化的失效。
Kaiming初始化针对使用ReLU的模型提出。因为ReLU会抛弃掉小于0的值，对于一个均值为0的输入数据，这就相当于砍掉了一半的值，这样输出的方差和均值就变了。因此把Xavier初始化中使用的方差sqrt(1/N)改为sqrt(2/N)，这样数据不会因为ReLU而变得越来越小。

### 4.使用Bert中的[CLS]作为输出进行分类和相似度计算，可能会有什么问题？

self-attention中，每个token都天然倾向于关注自己和附近的token，而对更远的内容分配较少的注意力。在Bert中，[CLS]一般是第一个token，放在最前面。
这就导致[CLS]更容易关注到靠近它的输入，也就是文本的靠前部分。如果文本较长，且关键内容出现在靠后的位置，就有可能出现[CLS]中关注不够的情况，而导致效果下降。
可以通过强制注意力分配，或者使用所有位置的输出而不单是[CLS]token的方法来缓解这个问题。

### 5.pytorch中的register_buffer是什么，有什么用？

register_buffer是nn.Module类中的一个方法，它用于注册一个不需要梯度的缓冲区。一般会把模型中需要持续使用或者跟踪，但是不需要通过梯度来更新的参数使用register_buffer注册。
使用register_buffer注册的参数可以和其他训练的参数一样，保存在state_dict中，比如旋转位置编码的旋转矩阵，batchnorm中的全局均值和方差，一般都会用register_buffer注册。如果不使用register_buffer注册，普通的常量或者参数保存的时候不会被state_dict跟踪到。
https://mp.weixin.qq.com/s/mCyCY1tWy6OjYjkByTjPqQ



### 1.MoE模型训练中，如果不对专家的路由进行适当干预，可能会遇到什么问题，有什么解决方法？
MoE使用多个并行的expert，每次推理只选择其中的一小部分expert使用。如果让模型完全自行学习，有可能出现routing collapse的问题，也就是模型倾向于总是选择那几个常用的专家。
而这些常用的专家由于使用得更多，训练得更好，又会提升被路由到的概率，导致大部分模型参数几乎没有被用上。一般可以通过增加一个负载平衡的loss来缓解。
负载平衡loss有不同的设计和计算方式，但是大致的思路都是迫使模型均匀地使用不同的专家，如果出现某些专家被选中的概率过高，就会进行惩罚。

### 2.Bert的预训练方式是MLM，通过[Mask] token对部分输入进行掩盖，要求模型预测。为什么要使用[Mask] token而不直接修改attention mask矩阵？
直接修改attention mask矩阵也可以让模型看不到对应位置的输入，但是相比使用[Mask] token缺少了位置编码的信息。另外使用[Mask] token掩盖要预测的值这种做法在实现上相对方便，只需要对输入数据进行处理即可，而不需要修改modeling的内容，更加通用。

### 3.为什么大模型训练的时候需要warmup？
在训练前期，刚随机初始化的模型参数离收敛值很远，此时的loss会比较大，梯度也会很大。如果直接使用固定的较大learning rate，模型容易过拟合到早期step见过的这些数据。
依照ResNet的实验结果，如果一开始模型就跑偏了，那后面再怎么训练，收敛效果都会比较差，说明早期太大的学习率导致模型过早收敛到不太好的局部最优了。
另外，模型刚开始训练的时候，大学习率带来的大更新值，会导致模型参数的震荡会很大，使得模型学到的参数很不稳定，这也不利于训练。使用少数step让模型进行热身，可以很大程度规避这些问题。

### 4.Roberta在Bert的基础上做了什么优化？
Bert在MLM训练中，提早把要mask的token处理好，在训练时训了多个epoch。这样在不同的epoch中，同一条sample总是使用相同的mask进行训练。
Roberta使用了dynamic mask，即每条数据在训练前才随机决定进行mask的位置，这样不同的epoch之间同一条样本也有不同的mask结果，提升了数据多样性。
Bert使用了MLM和NSP两个任务，而Roberta通过实验发现NSP的作用不大，因此直接取消了NSP任务。Roberta增大了batch size，提高训练效率，以获取更好的训练结果。
Roberta增大了训练数据和训练step数，实验表明模型继续训练还能进一步收敛。Bert使用WordPiece分词，而Roberta使用BBPE，增大了词表。再后来，Google发布了WWM全词mask，改进了mask方式。
Roberta-WWM也成了最广泛使用的版本。

### 5.LoRA的参数是怎么初始化的？
LoRA包含一个降维矩阵A，和一个升维矩阵B。矩阵A用随机高斯分布初始化，而矩阵B用初始化为0。这样可以使得训练开始的时候，LoRA的参数不产生效果，模型能够保持增加LoRA前的输出。但是A、B矩阵不能同时为0，这样会有对称性问题，降低了模型的表达能力。


https://mp.weixin.qq.com/s/uMv-RsawrB7h7qQ52VJfqg




### 1.激活函数GeLU是怎么设计的，有什么优点？

GeLU函数在2016年在论文《Gaussian Error Linear Units (GELUs)》中提出。之前比较常用的激活函数ReLU具有计算快，有激活值的时候不容易出现梯度消失等优点，但是ReLU是个分段函数，存在不可导的断点，可能会对模型效果有所影响。

此外，ReLU是确定性的激活函数，一般会加入随机正则项（如dropout，随机将部分输出置0）以提高模型的泛化能力。
而GeLU被设计为一种自带随机正则属性的激活函数，即输入是否置0，取决于当前的输入和其他输入的对比：GeLU(x) = xP(X≤x)=xΦ(x)其中Φ(x)是标准正态分布的累积分布函数
。GeLU精确值的计算比较复杂，因此论文给出了近似解，图像如图。GeLU相比ReLU更为光滑，在处理负数的时候也不会直接输出0，减少了梯度消失和神经元死亡的情况，实际使用中效果也较好。

### 2.Gshard和Switch Transformer对MoE模型都提出expert capacity的概念，即每个expert在一个batch中最多能处理的token数。设定不同的expert capacity，对模型会有什么影响？

expert capacity模型容量限定了一个batch中最多能处理的token数。如果一个token被分配到已经满载的expert，则这个token不会被处理，而是通过残差链接透传给下一层，这种情况称为overflow。overflow会对最终效果有损害，因为有些token没有被完整计算。
比如一个batch里有N个token，每层有E个expert，每个token被分配到2个expert，假设我们设置了expert capacity = 2N/E。
只有当所有token完美平衡分配才不会出现overflow。显然，expert capacity设置得越大，越能兼容分配不平衡的情况，overflow的情况越少；但是分配不平衡的情况会对模型的计算效率有损害，因为部分expert被频繁使用，而另外一部分可能使用很少，造成计算资源浪费。
下图展示了不同expert capacity下的情况

### 3.重计算recomputation是什么，有什么用？
通常来说，使用更大的模型或者更大的batch size可以获得更好的训练效果。但是更多的激活值也带来了显存不足的问题。
正常来说，为了进行反向传播，我们需要保存模型中每个节点的激活值，比如使用一个4层的模型，每层的和输出大小都为8，那么一共就要保存32个激活值。如果设备的显存只够储存24个值，这时候就会出现OOM。
一个解决方法就是，我们只保存第1和第3层的激活值，那么就只需要保存16个值。在计算第4层的反向传播时，先拿第3层的激活值重新计算第4层的前向结果，再进行正常反向传播；
在计算第2层的的反向传播时，先拿第1层的激活值重新计算第2层的前向结果，以此类推。这里重新计算第4层、第2层的前向结果的做法，就是重计算recomputation。
重计算可以节省显存，使得同样的设备可以训练更大的模型，但是由于需要多次计算前向结果，训练速度就会比较慢。因此重计算是用时间换空间的方法。

### 4.MoE模型中，使用更小的expert有什么优势？
更小的expert在同样总参数量和激活参数的情况下，可以提供更多的排列组合，使得每个expert在训练过程中能达到更高程度的专家化。
比如原来每层有16个专家，每个专家大小为128，每次激活两个，那么组合情况总共有120种不同的组合；如果使用64个专家，每个专家大小为32，每次激活8个，那么就有4,426,165,368种不同的组合。
另外，专家数量不够多的情况下，一个专家就可能要负责学习多个领域的内容。以学习高中知识为例，在只有两个专家的时候，只能一个专家学习理科知识，另一个学习文科知识；
当我们有8个专家的时候，不同专家就可以分别学习语文、英语、历史、地理、物理、生物、化学、数学知识。显然后者所学知识的专业化程度更高。

### 5.MoE模型中，使用的expert数量多少对模型有什么影响？
（1）专家的数量越多，同样的输入，给每个专家分配的平均token越少。这在训练时可能导致batch size过小，影响训练效果。
（2）专家的数量越多，训练和推理时所需的设备越多，多个设备进行通讯的成本会变高。
（3）专家的数量如果比较少，提供的组合数就比较少；专家数量多的话可以提供更多可能的组合。
（4）专家的数量多，模型的总参数量更大，理论上模型能具有更大的容量，上限更高。


https://mp.weixin.qq.com/s/D3Km3X_Hvq-bpeMfP8WbAQ


### 1.大模型训练过程中有什么可以缓解显存不足的办法？
（1）模型结构：使用LoRA、adaptor等训练
（2）注意力计算底层优化：flash attention、paged attention、ring attention等
（3）训练框架：使用混合精度训练，使用ZeRO、recomputation、cpu-offload等
（4）训练策略：梯度累积

### 2.为什么MoE模型的训练相比dense模型，更容易受到精度不足的影响？
模型训练中经常使用的float32/float16/bfloat16等，都存在舍入误差，并且表达的数值越大，舍入误差越大。
而MoE模型中，gating function大量使用了exponential计算，而exponential会把输入数据中的误差放大很多倍，从而使得输出结果大大偏离。
因此有些工作会限制在gating function使用更高精度的表达，并通过一些前置手段压缩数值的大小，从而缓解误差在多个MoE层累计的情况。

### 3.模型蒸馏中，为什么要使用温度T？
经典的模型蒸馏过程中，student model会学习teacher model的softmax输出，在teacher model的输出进入softmax前，会除以一个温度T>1。
经过较好训练的teacher model一般会以较高的置信度给出最终结果，而我们使用蒸馏而不从groud truth（即one-hot label）直接学习的原因，是因为除了置信度最高的结果之外，其他类别的分值也能够提供有价值的信息。
比如在水果图片三分类任务，共有三个类别 [菠萝，榴莲，甘蔗]，teacher model 对一张图片给出 [0.7, 0.2, 0.1] 的结果，除了正确答案0.7以外，0.2和0.1这两个分支提供了一个信息：榴莲和菠萝的相似程度大于甘蔗和菠萝，这是one-hot label提供不了的。
而除以温度T，能够让teacher model给出的label变得更加soft，信息熵更大，从而可以让student model更好地学习。

### 4.大模型有什么加速推理的方法？
1.针对训练好的模型：模型蒸馏、模型量化、模型剪枝等
2.模型结构设计：MoE
3.训练策略：early-exit
4.解码策略：投机解码

### 5.为什么LLAMA1/2的intermiedia size/hidden size相比Bert的减小了，从4下降到8/3？

LLAMA使用SwiGLU FFN，相比Bert的FFN，多了一个线性变化矩阵。为了保持模型的参数量不变，因此把intermediate size从4倍hidden size降为8/3倍。

https://mp.weixin.qq.com/s/CJNKMCeIFZSo0zZG1yyF6Q























