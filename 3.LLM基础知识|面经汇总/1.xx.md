

#### prefix LM 和 causal LM 区别是什么?

prefix LM (前缀语言模型)：在输入序列的开头添加一个可学习的任务相关的前缀，然后使用这个前缀和输入序列一起生成输出。这种方法可以引导模型生成适应特定任务的输出。causal LM (因果语言模型)：也称为自回归语言模型，它根据之前生成的 token 预测下一个token。在生成文本时，模型只能根据已经生成的部分生成后续部分，不能访问未来的信息。


#### prefix LM 和 causal LM、encoder-decoder 区别及各自有什么优缺点?

prefix LM：通过在输入序列前添加可学习的任务相关前缀，引导模型生成适应特定任务的输 出。优点是可以减少对预训练模型参数的修改，降低过拟合风险；缺点是可能受到前缀表示长度的限制，无法充分捕捉任务相关的信息。

causal LM：根据之前生成的 token预测下一个 token, 可以生成连贯的文本。优点是可以生成灵 活的文本，适应各种生成任务；缺点是无法访问未来的信息，可能生成不一致或有误的内容。

encoder-decoder：由编码器和解码器组成，编码器将输入序列编码为固定长度的向量，解码器 根据编码器的输出生成输出序列。优点是可以处理输入和输出序列不同长度的任务，如机器翻译；缺点是模型结构较为复杂，训练和推理计算量较大。

#### 大模型的Tokenizer的实现方法及原理?

大模型的Tokenizer通常使用字节对编码 (Byte-Pair Encoding,BPE) 算法。BPE算法通过迭代地将最频繁出现的字节对合并成新的符号，来构建一个词汇表。在训练过程中，模型会学习这些符号的嵌入表示。Tokenizer将输入文本分割成符号序列，然后将其转换为模型可以处理的数字表示。


#### ChatGLM3的词表实现方法?

ChatGLM3 使用了一种改进的词表实现方法。它首先使用字节对编码 (BPE) 算法构建一个基本的词表，然后在训练过程中通过不断更新词表来引入新的词汇。具体来说，ChatGLM3 在训练 过程中会根据输入数据动态地合并出现频率较高的字节对，从而形成新的词汇。这样可以有效地处理大量文本数据，并减少词汇表的规模。同时，ChatGLM3 还使用了一种特殊的词表分割方法，将词表分为多个片段，并在训练过程中逐步更新这些片段，以提高模型的泛化能力和适应性。


#### GPT3、LLAMA、ChatGLM 的 Layer Normalization 的区别是什么?各自的优缺点是什么?

GPT3：采用了Post-Layer Normalization (后标准化)的结构，即先进行自注意力或前馈神经网络的计算，然后进行Layer Normalization。这种结构有助于稳定训练过程，提高模型性能。

LLAMA：采用了Pre-Layer Normalization (前标准化)的结构，即先进行Layer Normalization,然后进行自注意力或前馈神经网络的计算。这种结构有助于提高模型的泛化能力和鲁棒性。

ChatGLM：采用了Post-Layer Normalization的结构，类似于GPT3。这种结构可以提高模型的性能和稳定性。


#### 大模型常用的激活函数有哪些？

ReLU（Rectified Linear Unit）：一种简单的激活函数，可以解决梯度消失问题，加快训练速度。
GeLU（Gaussian Error Linear Unit）：一种改进的ReLU函数，可以提供更好的性能和泛化能力。
Swish：一种自门控激活函数，可以提供非线性变换，并具有平滑和非单调的特性。


#### 多查询注意力与群查询注意力是否了解?区别是什么?

Multi-query Attention 和 Grouped-query Attention 是两种不同的注意力机制变种，用于改进和扩展传统的自注意力机制。

Multi-query Attention：在Multi-query Attention中，每个查询可以与多个键值对进行交互，从而 捕捉更多的上下文信息。这种机制可以提高模型的表达能力和性能，特别是在处理长序列或复杂关系时。

Grouped-query Attention：在Grouped-query Attention中，查询被分成多个组，每个组内的查询与对应的键值对进行交互。这种机制可以减少计算复杂度，提高效率，同时仍然保持较好的性能。
